# =============================================================================
# Stage 1: WebVid Captioning
# =============================================================================
# First training stage -- bootstraps vision-language alignment from noisy
# WebVid captions.  Loss on ALL text tokens (not answer-only) because the
# entire caption is the "answer" for WebVid data.
#
# Training recipe follows SmolVLM2 Stage 1:
#   * All parameters trainable (differential LR: high connector, low backbone)
#   * 14% text-only data interleaved to preserve instruction-following
#   * 1 FPS, variable frame count, cap at 64 frames (matches SmolVLM2)
#   * WebVid captions are noisy -- formatted as
#     "What would be the WebVid caption for this video?"
#
# Expected runtime: ~12h on 2xA100-80GB (batch_size 16, grad_accum 2)
# =============================================================================

stage: 1

model:
  llm: /workspace/models/SmolLM2-135M-Instruct
  dino: /workspace/models/dinov2-small
  deep_query: true
  query_dim: 384
  visual_scale: 0.14
  lambda_coarse: 0.0
  gradient_checkpointing: false

data:
  # OpenVid-1M replaces dead WebVid URLs. Glob matches 6-digit shard names.
  train_shards: "/workspace/data/openvid/*.tar"
  val_shards: "/workspace/data/eval/val_10k/*.tar"
  text_shards: "/workspace/data/text_retention/stage1/*.tar"
  text_ratio: 0.14
  max_frames: 64
  frame_size: 224
  num_workers: 4

training:
  total_samples: 1_000_000  # ~1M from OpenVid (186 parts Ã— ~5.5K each)
  batch_size: 16
  grad_accum: 2
  lr_connector: 1.0e-4
  lr_dino: 1.0e-5
  lr_llm: 1.0e-5
  warmup_ratio: 0.05
  weight_decay: 0.01
  max_grad_norm: 1.0
  schedule: cosine
  dtype: bfloat16
  compile: true
  seed: 42

loss:
  type: text_ce_all  # Stage 1: loss on ALL text tokens

checkpoint:
  save_dir: /workspace/checkpoints/stage1
  save_every_steps: 1000
  keep_last: 2
  keep_best: 1
  metric: val_loss
  resume: auto

eval:
  every_steps: 500
  max_samples: 1000

wandb:
  project: foveated-vlm-stage1
