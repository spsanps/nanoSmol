# System Migration Handoff — 2026-02-16

**Purpose:** Complete state dump for continuing fVLM project on a new system. Written by LOCAL Claude Code during system migration.

---

## 1. Project Overview

**fVLM** = Foveated Vision-Language Model. 1 token per video frame via query-guided cross-attention on DINOv2 features. The LLM generates dynamic queries that control WHERE to look — like biological foveated vision.

**Thesis:** 1 foveated token per frame matches 16 standard tokens at same compute budget.

**Repo:** `github.com/spsanps/nanoSmol` → `fVLM/` subfolder. Canonical code in `release/`.

---

## 2. Infrastructure

### Local Machine
- Windows 11 + WSL2 (Linux 5.15.133.1)
- Working directory: `/mnt/c/Users/sanps/Desktop/projects/nanoSmol/fVLM`
- SSH key: `/mnt/c/Users/sanps/.ssh/id_ed25519`

### RunPod GPU Pod (CURRENTLY RUNNING)
- **GPU:** 1x NVIDIA RTX 5090, 32GB VRAM
- **Cost:** ~$0.89/hr, ~$400 remaining of $500 budget
- **SSH:** `ssh -i ~/.ssh/id_ed25519 root@213.173.111.196 -p 38297`
- **VS Code SSH:** Host entries `RUNPOD` (worker) and `RUNPOD-ROOT` in `~/.ssh/config`
- **Network volume:** `/workspace/` persists across pods (506GB used / 1TB)
- **Users:** `root` (default) + `worker` (for Claude Code, password-less)
- **Python env:** `/workspace/venv311/bin/activate`, torch 2.10.0+cu128
- **Code on pod:** `/workspace/workdir/nanoSmol/fVLM`
- **Git remote on pod:** SSH (`git@github.com:spsanps/nanoSmol.git`), deploy key configured

### Inter-Claude Communication
- Board file: `/workspace/comms/BOARD.md` (append-only, timestamped)
- Polling from local: `/tmp/poll_comms.sh` (checks every 60s via SSH)
- Tags: `[LOCAL]` vs `[GPU]` prefixes

---

## 3. Git State

### Local repo (main branch)
```
06a1723 HEAD → main (behind origin/main by ~11 commits)
```

### GitHub (origin/main)
```
236213d perf: nanochat-inspired optimizations + disable compile
f3d3af4 perf: skip padded frames in DINO + TF32 + batch_size=8
f82d05e config: adjust all ablation configs for RTX 5090 32GB
887a238 fix: glob expansion for webdataset shard patterns + encoder tuple handling
6e38ddf fix: 4 bugs — dtype mismatch, torch_dtype param, CSV header
f869189 docs: Update BRIEFING.md for RTX 5090 GPU pod setup
... (more docs commits)
```

### GPU Pod (5 UNPUSHED commits on top of 236213d)
```
483c27d feat: Phase 1b scaling grid template + runner improvements   ← HEAD
ae6e8ee feat: add ablation analysis script for Phase 1a results comparison
adfcd91 config: add A7, B2 configs + fix B1 batch_size for OOM
f1b787c fix: implement shallow query mode (deep_query=false was a no-op)
5588aed perf: batched DINO query + SDPA — 2.8x training speedup
236213d perf: nanochat-inspired optimizations + disable compile      ← origin/main
```

**ACTION NEEDED:** Push the 5 commits from GPU pod to GitHub before doing anything else:
```bash
ssh root@213.173.111.196 -p 38297 "cd /workspace/workdir/nanoSmol/fVLM && git push origin main"
```
Then pull locally: `git pull origin main`

---

## 4. Phase 1a Ablations — COMPLETE ✓

All 13 foveated ablation runs finished. 300K samples each, SmolLM2-135M, seed 42.

### Results (sorted by validation loss)

| Rank | Run | Config | Val Loss | Wall Time | Key Finding |
|------|-----|--------|----------|-----------|-------------|
| 1 | D1 video_heavy | D1_video_heavy.yaml | **1.312** | 1.93h | More video data → biggest win |
| 2 | LR2 (100:1) | LR2.yaml | **1.332** | 1.76h | Aggressive connector LR helps significantly |
| 3 | A1v2 (shallow+fine) | A1_deep_query_off.yaml | 1.349 | 1.68h | Shallow nearly matches deep |
| 4 | LR3 (3:1) | LR3.yaml | 1.351 | 1.78h | Lower ratio still better than baseline |
| 5 | baseline (deep+fine) | baseline.yaml | 1.361 | 1.73h | Reference point |
| 6 | A6 (deep+coarse) | A6_coarse_only.yaml | 1.369 | 1.19h | Fine pass adds ~0.008 |
| 7 | A7 (shallow+coarse) | A7_shallow_coarse_only.yaml | 1.373 | 1.19h | Worst foveated variant |
| 8 | LR4 (1:1) | LR4.yaml | ~1.37 | 1.79h | Equal LR = worse |
| 9 | A8 16frames | A8_static_16frames.yaml | — | 2.27h | Frame replication for images |
| 10 | A8 1frame | A8_static_1frame.yaml | — | 1.17h | Single frame baseline for images |
| 11 | F1 freeze_both | F1_freeze_both.yaml | 1.999 | 1.27h | Much worse — full fine-tuning essential |
| 12 | F2 freeze_llm | F2_freeze_llm.yaml | 1.938 | 1.53h | Much worse — LLM must train |

### Decisions Made
- **Architecture:** Keep deep+fine (full two-pass). Deep query + fine pass likely interact synergistically.
- **LR:** 100:1 connector-to-backbone ratio (connector=1e-3, backbone=1e-5)
- **Data mix:** Video-heavy preferred (D1 was best)
- **Freeze:** Full fine-tuning essential (frozen variants much worse)

### Checkpoints & Logs
- Checkpoints: `/workspace/checkpoints/ablations/{run_name}/`
- Run summaries: `run_summary_*.json` in each checkpoint dir
- Metrics CSVs: `metrics_*.csv` in each checkpoint dir
- Master results: `/workspace/ablation_logs/ablation_results.csv`

### 2×2 Factorial Analysis (deep_query × fine_pass)
```
                  coarse_only    coarse+fine
deep_query:       A6 (1.369)     baseline (1.361)     Δ = 0.008
shallow_query:    A7 (1.373)     A1v2 (1.349)         Δ = 0.024
```
Shallow+fine (A1v2) unexpectedly best within factorial. But deep+fine chosen for theoretical reasons (foveation thesis requires deep queries at scale).

---

## 5. Phase 1b Scaling Grid — FAILED (easy fix)

### What Happened
The scaling grid runner (`run_scaling_grid.py`) launched, and the FIRST run (135M-C1-F) actually **trained successfully** — but the script crashed when writing results to CSV.

### The Bug
```
ValueError: dict contains fields not in fieldnames: 'returncode'
```
At `release/scripts/run_scaling_grid.py:250`, the `result` dict from `run_training()` contains `returncode` but the CSV fieldnames list doesn't include it.

### Fix
In `release/scripts/run_scaling_grid.py`, add `'returncode'` to the fieldnames list at line 228-232:
```python
writer = csv.DictWriter(f, fieldnames=[
    "run_id", "size", "budget", "arch", "flop_budget",
    "total_samples", "status", "returncode", "wall_time_sec",
    "final_train_loss", "best_val_loss", "best_val_step",
])
```

### 135M-C1-F Results (VALID — already trained)
- Val loss: 1.4178
- Total samples: 67,000
- Wall time: 1.03h
- Checkpoint: `/workspace/checkpoints/scaling/135M-C1-F/`
- Summary: `run_summary_135M-C1-F_20260216_080531.json`

### Remaining Scaling Runs
After fix, re-run the grid but **skip 135M-C1-F** (already done):
```bash
python release/scripts/run_scaling_grid.py \
    --template release/configs/scaling/template.yaml \
    --filter "135M-C2"  # then C3, C4, then 360M runs
```

Or run all and let it overwrite (idempotent):
```bash
python release/scripts/run_scaling_grid.py \
    --template release/configs/scaling/template.yaml
```

### Grid Configuration
- Template: video-heavy data + 100:1 connector LR + deep+fine
- Sizes: 135M (C1-C4), 360M (C1-C4)
- FLOP budgets: C1=1.6e16, C2=5.6e16, C3=1.6e17, C4=3.1e17
- **1.7B deferred to A100** (won't fit on 32GB RTX 5090)
- ETA: ~30h total (135M ~13h + 360M ~17h)

---

## 6. Known Bugs (all fixed)

| Bug | File | Fix | Status |
|-----|------|-----|--------|
| BUG-001: q_static init too small | encoder.py | std=1.0 (was 0.02) | Fixed pre-GPU |
| BUG-002: Bias on query_input_proj | encoder.py | bias=False | Fixed pre-GPU |
| BUG-003: Per-sample mode selection | foveated_vlm.py | Per-batch | Fixed pre-GPU |
| BUG-004: Shallow = uniform attention | encoder.py | deep_query=True default | Fixed pre-GPU |
| BUG-005: torch_dtype param name | foveated_vlm.py, multi_token_vlm.py | `torch_dtype=` not `dtype=` | Fixed (6e38ddf) |
| BUG-006: Loss mask dtype mismatch | foveated_vlm.py line 314, multi_token_vlm.py line 192 | Use `attention_mask.dtype` | Fixed (6e38ddf) |
| BUG-007: CSV header rewrite | run_scaling_grid.py | Remove `or i == 0` | Fixed (6e38ddf) |
| BUG-008: Glob pattern expansion | webdataset_loader.py | Expand globs before wds | Fixed (887a238) |
| BUG-009: DINOv2 layer tuple | encoder.py | `layer_out[0] if isinstance(...)` | Fixed (887a238) |
| BUG-010: deep_query=False no-op | encoder.py | Implement `shallow_query_attend()` | Fixed (f1b787c) |
| BUG-011: CSV returncode field | run_scaling_grid.py | Add 'returncode' to fieldnames | **NOT YET FIXED** |

---

## 7. Optimization Lessons (from LESSONS_LEARNED.md on pod)

| Optimization | Result |
|-------------|--------|
| torch.compile | **HURTS** with dynamic shapes (frame_mask). +7% by NOT compiling |
| Gradient checkpointing | No help at 157M params (recompute overhead = batch size gains) |
| Frame mask (skip padding) | **65% DINO compute savings**, enabled batch_size 4→8 |
| Batched DINO query + SDPA | **2.8x speedup** |
| fused AdamW, gc.disable() | Small cumulative gains |
| TF32 matmul | Free performance on Ampere+ |
| RTX 5090 batch_size | 8 + grad_accum=4 (effective 32). Peak VRAM: 27.6GB |
| Throughput | 55-77 samp/s frozen, 16-18 samp/s unfrozen |

---

## 8. Data & Models on /workspace/

### Models (41GB, /workspace/models/)
| Model | Size | Purpose |
|-------|------|---------|
| SmolLM2-135M-Instruct | — | Ablation workhorse |
| SmolLM2-360M-Instruct | — | Scaling midpoint |
| SmolLM2-1.7B-Instruct | — | Scaling upper (needs A100) |
| dinov2-small | 384d | Vision encoder |
| dinov2-base | 768d | Optional larger encoder |
| SmolVLM2-256M-Video-Instruct | — | Eval baseline |
| SmolVLM2-2.2B-Instruct | — | Eval baseline |

### Training Data (450GB, /workspace/data/)
| Dataset | Shards | Samples | Stage |
|---------|--------|---------|-------|
| Cauldron | 2,001 | 2.0M | 1,2,3 |
| OpenVid-1M | 905 | 905K | 1 |
| Vript long | 400 | 398K | 1,3 |
| LLaVA-Video | 266 | 266K | 2 |
| SmolTalk S1 | 280 | 280K | 1 |
| SmolTalk S2 | 140 | 140K | 2 |
| VISTA main | 163 | 146K | 2,3 |
| SmolTalk S3 | 70 | 70K | 3 |
| VISTA extra | 66 | 58K | 2,3 |
| RLAIF-V DPO | 84 | 84K | 3(opt) |
| ShareGPT4Video | 61 | 61K | 1 |
| Stage3 video | 50 | 50K | 3 |
| Stage3 YouTube | 22 | 22K | 3 |
| WebVid | 19 | 19K | 1 |
| Vript short | 11 | 11K | 1,3 |
| Eval val_10k | 10 | 10K | Eval |

All shards use unified `{"user": "...", "assistant": "...", "source": "...", "frame_count": N}` JSON format.

---

## 9. What To Do Next (in order)

### Immediate (before any training)
1. **Push 5 unpushed commits** from GPU pod to GitHub
2. **Fix BUG-011** — add `'returncode'` to CSV fieldnames in `run_scaling_grid.py`
3. **Pull** latest to local: `git pull origin main`

### Phase 1b: Scaling Grid (~30h, ~$27)
4. **Re-run scaling grid** with the fix. 135M-C1-F is done, need C2-C4 + all 360M runs.
5. **1.7B runs** — switch to A100 pod temporarily (1.7B won't fit on 32GB RTX 5090)

### Phase 1b Analysis
6. **Plot scaling curves** — val_loss vs FLOPs for each model size
7. **Determine optimal model size** for full training budget

### Full 3-Stage Training (~$250)
8. **Stage 1** (WebVid captioning) with winning config
9. **Stage 2** (VL SFT on Cauldron)
10. **Stage 3** (Video SFT)

### Open Design Questions
- **B2 baseline:** DINOv2 + pixel shuffle → ~49 tokens/frame (SmolVLM2-style connector, same encoder for fair comparison). For IMAGE iso-FLOP comparison only, NOT video.
- **Multi-token baselines** (B1/B2): Only for image experiments. For video, more frames at 1 tok/frame is the right comparison.
- **A8 frame replication results:** Need to analyze 1-frame vs 16-frame results for image handling in training.

---

## 10. Key Reference Documents

| Document | Purpose |
|----------|---------|
| `CLAUDE.md` | Project guide, architecture, paths, decisions |
| `docs/runpod/BRIEFING.md` | GPU pod context for fresh Claude instances |
| `docs/GPU_PHASE1_PLAN.md` | THE execution plan (ablations, scaling, metrics) |
| `docs/KNOWLEDGE.md` | All experiment history, bugs, data pipeline details |
| `docs/SCALING_PLAN.md` | 3-stage training plan, scaling study design |
| `docs/runpod/SMOLVLM2_REFERENCE.md` | SmolVLM2 training details for comparison |
| `core_docs/foveated_vlm_proposal.md` | Original architecture spec |
| `/workspace/LESSONS_LEARNED.md` | GPU optimization lessons (on pod only) |
| `/workspace/comms/BOARD.md` | Full chronological history of both Claudes |

---

## 11. Architecture Decisions (DO NOT CHANGE)

| Decision | Value | Why |
|----------|-------|-----|
| deep_query | True | Shallow = uniform attention = no foveation |
| query_dim | 384 | Matches DINO-small hidden dim |
| bias on query_input_proj | False | Bias dominated signal |
| q_static/q_init init std | 1.0 | 0.02 killed gradients |
| Mode selection | Per-batch | Per-sample broke DDP sync |
| visual_scale | 0.14 | Matches LLM embedding std |
| Loss | Text CE only | No reconstruction, no VAE |
| Text retention | 14% SmolTalk | Removing hurts 3.7-6.5% |
| Frame rate | 1 FPS, cap 64 | Matches SmolVLM2 |
| Frame size | 224x224 | DINOv2-small native |
| Connector LR | 100:1 ratio | Ablation winner (LR2) |
| Data mix | Video-heavy | Ablation winner (D1) |

---

## 12. SSH Config (for reference)

```
# In ~/.ssh/config
Host RUNPOD
  HostName 213.173.111.196
  User worker
  Port 38297
  IdentityFile C:/Users/sanps/.ssh/id_ed25519

Host RUNPOD-ROOT
  HostName 213.173.111.196
  User root
  Port 38297
  IdentityFile C:/Users/sanps/.ssh/id_ed25519
```

---

*Written: 2026-02-16 by LOCAL Claude Code during system migration.*
