# =============================================================================
# Ablation LR4: Uniform LR (No Differential Learning Rates)
# =============================================================================
# connector=1e-4, dino=1e-4, llm=1e-4
# ALL parameter groups at the same learning rate.
#
# Change from baseline:
#   lr_dino: 1e-5  -->  1e-4
#   lr_llm:  1e-5  -->  1e-4
#
# Key question: Is differential learning rate even needed?
#
# Conventional VLM wisdom says newly-initialized connectors need higher LR
# than pretrained backbones.  But maybe for small models (135M LLM, 22M
# DINO-small) the difference matters less.  If uniform LR matches
# differential, we can simplify the training pipeline.
#
# Risk: Pretrained weights may be destabilised at 1e-4.  Idefics3 diverged
# at 8B with full unfreezing -- but we are much smaller.  Monitor loss
# carefully in first 500 steps.
#
# Expected runtime: ~2h on 2xA100-80GB
# =============================================================================

stage: 1

model:
  llm: /workspace/models/SmolLM2-135M-Instruct
  dino: /workspace/models/dinov2-small
  deep_query: true
  query_dim: 384
  visual_scale: 0.14
  lambda_coarse: 0.0
  gradient_checkpointing: false

data:
  train_shards: "/workspace/data/webvid/{00000..00359}.tar"
  val_shards: "/workspace/data/eval/val_10k/*.tar"
  text_shards: "/workspace/data/text_retention/stage1/*.tar"
  text_ratio: 0.14
  max_frames: 64
  frame_size: 224
  num_workers: 4

training:
  total_samples: 360_000
  batch_size: 16
  grad_accum: 2
  lr_connector: 1.0e-4    # <-- ABLATION: uniform across all groups
  lr_dino: 1.0e-4         # <-- ABLATION: 10x higher than baseline
  lr_llm: 1.0e-4          # <-- ABLATION: 10x higher than baseline
  warmup_ratio: 0.05
  weight_decay: 0.01
  max_grad_norm: 1.0
  schedule: cosine
  dtype: bfloat16
  compile: true
  seed: 42

loss:
  type: text_ce_all

checkpoint:
  save_dir: /workspace/checkpoints/ablations/LR4
  save_every_steps: 500
  keep_last: 2
  keep_best: 1
  metric: val_loss
  resume: auto

eval:
  every_steps: 250
  max_samples: 1000

wandb:
  project: foveated-vlm-ablations
  run_name: LR4_uniform
