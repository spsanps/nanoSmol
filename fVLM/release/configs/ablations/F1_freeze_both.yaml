# =============================================================================
# Ablation F1: Freeze Both (Connector Only Training)
# =============================================================================
# freeze_dino: true, freeze_llm: true
# Only the connector (dino_to_llm, llm_to_query, q_static, q_init) is trained.
#
# Change from baseline:
#   freeze_dino: false  -->  true
#   freeze_llm:  false  -->  true
#
# Tests the minimal training scenario: can the connector alone bridge DINO
# and LLM?  If F1 â‰ˆ F3 (baseline), backbone unfreezing is unnecessary.
# If significantly worse, backbone adaptation is required.
#
# Expected runtime: ~3h on 2xA100-80GB (faster -- no backbone gradients)
# =============================================================================

stage: 1

model:
  llm: /workspace/models/SmolLM2-135M-Instruct
  dino: /workspace/models/dinov2-small
  deep_query: true
  query_dim: 384
  visual_scale: 0.14
  lambda_coarse: 0.0
  freeze_dino: true              # <-- ABLATION: DINO frozen
  freeze_llm: true               # <-- ABLATION: LLM frozen
  gradient_checkpointing: false

data:
  train_shards: "/workspace/data/openvid/*.tar"
  val_shards: "/workspace/data/eval/val_10k/*.tar"
  text_shards: "/workspace/data/text_retention/stage1/*.tar"
  text_ratio: 0.14
  max_frames: 64
  frame_size: 224
  num_workers: 8

training:
  total_samples: 300_000
  batch_size: 8
  grad_accum: 4
  lr_connector: 1.0e-4
  lr_dino: 1.0e-5
  lr_llm: 1.0e-5
  warmup_ratio: 0.05
  weight_decay: 0.01
  max_grad_norm: 1.0
  schedule: cosine
  dtype: bfloat16
  compile: false
  seed: 42

loss:
  type: text_ce_all

checkpoint:
  save_dir: /workspace/checkpoints/ablations/F1_freeze_both
  save_every_steps: 1000
  keep_last: 2
  keep_best: 1
  metric: val_loss
  resume: auto

eval:
  every_steps: 200
  max_samples: 1000

wandb:
  project: foveated-vlm-ablations
  run_name: F1_freeze_both
