# =============================================================================
# Baseline B1: Multi-Token (16 tokens/frame, no foveation)
# =============================================================================
# Standard VLM approach: DINOv2 patches → 4x4 pool → 16 tokens → project → LLM.
# No query mechanism, no two-pass, no foveated attention.
#
# This is the KEY efficiency comparison:
#   - Foveated (F3): 1 token/frame, ~1x DINO FLOPs
#   - Multi-token (B1): 16 tokens/frame, ~16x more LLM visual tokens
#
# Same DINO, same LLM, same data. Only difference: visual token count.
#
# If B1 with 16x more tokens only marginally beats F3 → efficiency win proven.
# If B1 clearly beats F3 → foveation trades quality for efficiency (still publishable).
#
# Expected runtime: ~7h on 2xA100-80GB (16x more visual tokens → longer sequences)
# =============================================================================

stage: 1

model:
  llm: /workspace/models/SmolLM2-135M-Instruct
  dino: /workspace/models/dinov2-small
  multi_token: true              # <-- USE MultiTokenVLM instead of FoveatedVLM
  tokens_per_frame: 16           # <-- 4x4 pool of DINOv2 patches
  visual_scale: 0.14
  gradient_checkpointing: false

data:
  train_shards: "/workspace/data/openvid/*.tar"
  val_shards: "/workspace/data/eval/val_10k/*.tar"
  text_shards: "/workspace/data/text_retention/stage1/*.tar"
  text_ratio: 0.14
  max_frames: 64
  frame_size: 224
  num_workers: 4

training:
  total_samples: 1_000_000
  batch_size: 16
  grad_accum: 2
  lr_connector: 1.0e-4
  lr_dino: 1.0e-5
  lr_llm: 1.0e-5
  warmup_ratio: 0.05
  weight_decay: 0.01
  max_grad_norm: 1.0
  schedule: cosine
  dtype: bfloat16
  compile: false                 # longer sequences, may need gradient checkpointing instead
  seed: 42

loss:
  type: text_ce_all

checkpoint:
  save_dir: /workspace/checkpoints/ablations/B1_multi_token
  save_every_steps: 500
  keep_last: 2
  keep_best: 1
  metric: val_loss
  resume: auto

eval:
  every_steps: 250
  max_samples: 1000

wandb:
  project: foveated-vlm-ablations
  run_name: B1_multi_token
