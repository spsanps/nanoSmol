# =============================================================================
# Scaling Grid Template
# =============================================================================
# Template configuration for scaling law experiments.  Each run in the
# scaling grid substitutes concrete values for the {{PLACEHOLDER}} fields.
#
# The scaling grid explores:
#   * 3 LLM sizes:   SmolLM2-135M, SmolLM2-360M, SmolLM2-1.7B
#   * 2 DINO sizes:  dinov2-small (384d, 22M), dinov2-base (768d, 86M)
#   * Multiple FLOP budgets (iso-FLOP: smaller models train longer)
#
# To instantiate a run, copy this template and replace all {{...}} fields.
# Or use the scaling runner script which does this programmatically.
#
# Example instantiation (135M + dinov2-small, 360K samples):
#   model.llm: /workspace/models/SmolLM2-135M-Instruct
#   model.dino: /workspace/models/dinov2-small
#   model.query_dim: 384
#   training.total_samples: 360_000
#   training.batch_size: 16
#   checkpoint.save_dir: /workspace/checkpoints/scaling/135M_dino_s_360k
#   wandb.run_name: scaling_135M_dino_s_360k
#
# FLOP budget formula (approximate, per sample):
#   FLOPs = 2 * (dino_params + llm_params + connector_params) * tokens_per_sample
#   Iso-FLOP: total_samples = FLOP_budget / FLOPs_per_sample
#   Smaller models see more samples at the same FLOP budget.
# =============================================================================

stage: 1

model:
  # --- Substitute with actual model path ---
  llm: "{{LLM_PATH}}"                  # /workspace/models/SmolLM2-{135M,360M,1.7B}-Instruct
  dino: "{{DINO_PATH}}"                # /workspace/models/dinov2-{small,base}
  deep_query: true
  query_dim: "{{QUERY_DIM}}"           # 384 for dinov2-small, 768 for dinov2-base
  visual_scale: 0.14
  lambda_coarse: 0.0
  gradient_checkpointing: "{{GRAD_CKPT}}"  # false for 135M/360M, true for 1.7B

data:
  train_shards: "/workspace/data/openvid/*.tar"
  val_shards: "/workspace/data/eval/val_10k/*.tar"
  text_shards: "/workspace/data/text_retention/stage1/*.tar"
  text_ratio: 0.14
  max_frames: 64
  frame_size: 224
  num_workers: 4

training:
  total_samples: "{{TOTAL_SAMPLES}}"   # Varies by FLOP budget and model size
  batch_size: "{{BATCH_SIZE}}"         # 16 for 135M, 8 for 360M, 4 for 1.7B
  grad_accum: "{{GRAD_ACCUM}}"         # Adjusted so effective_batch stays ~32
  lr_connector: 1.0e-4
  lr_dino: 1.0e-5
  lr_llm: 1.0e-5
  warmup_ratio: 0.05
  weight_decay: 0.01
  max_grad_norm: 1.0
  schedule: cosine
  dtype: bfloat16
  compile: true
  seed: 42

loss:
  type: text_ce_all

checkpoint:
  save_dir: "/workspace/checkpoints/scaling/{{RUN_NAME}}"
  save_every_steps: 500
  keep_last: 1
  keep_best: 1
  metric: val_loss
  resume: auto

eval:
  every_steps: 250
  max_samples: 1000

wandb:
  project: foveated-vlm-scaling
  run_name: "{{RUN_NAME}}"

# ---------------------------------------------------------------------------
# Reference: Model sizes and memory estimates
# ---------------------------------------------------------------------------
# | LLM             | Params | VRAM (bf16) | batch_size | grad_accum |
# |-----------------|--------|-------------|------------|------------|
# | SmolLM2-135M    |  135M  |    ~6GB     |     16     |      2     |
# | SmolLM2-360M    |  360M  |   ~12GB     |      8     |      4     |
# | SmolLM2-1.7B    |  1.7B  |   ~40GB     |      4     |      8     |
#
# | DINO            | Params | VRAM (bf16) | query_dim |
# |-----------------|--------|-------------|-----------|
# | dinov2-small    |   22M  |    ~1GB     |    384    |
# | dinov2-base     |   86M  |    ~3GB     |    768    |
#
# Iso-FLOP example (budget = 1e18 FLOPs):
#   135M model: ~2.0M samples
#   360M model: ~0.8M samples
#   1.7B model: ~0.17M samples
# ---------------------------------------------------------------------------
