# Scaling: SmolLM2-1.7B + DINOv2-base
# Total params: ~1.79B. Large LLM, larger vision encoder.
# Iso-FLOP: ~30K samples. Gradient checkpointing required.
# query_dim=768 (DINOv2-base embedding dimension)

stage: 1

model:
  llm: /workspace/models/SmolLM2-1.7B-Instruct
  dino: /workspace/models/dinov2-base
  deep_query: true
  query_dim: 768
  visual_scale: 0.14
  lambda_coarse: 0.0
  gradient_checkpointing: true  # Required for 1.7B on 80GB

data:
  train_shards: "/workspace/data/webvid/{00000..00359}.tar"
  val_shards: "/workspace/data/eval/val_10k/*.tar"
  text_shards: "/workspace/data/text_retention/stage1/*.tar"
  text_ratio: 0.14
  max_frames: 64
  frame_size: 224
  num_workers: 4

training:
  total_samples: 30_000
  batch_size: 4
  grad_accum: 8
  lr_connector: 1.0e-4
  lr_dino: 1.0e-5
  lr_llm: 1.0e-5
  warmup_ratio: 0.05
  weight_decay: 0.01
  max_grad_norm: 1.0
  schedule: cosine
  dtype: bfloat16
  compile: true
  seed: 42

loss:
  type: text_ce_all

checkpoint:
  save_dir: /workspace/checkpoints/scaling/1700M_dino_b
  save_every_steps: 200
  keep_last: 1
  keep_best: 1
  metric: val_loss
  resume: auto

eval:
  every_steps: 100
  max_samples: 500

wandb:
  project: foveated-vlm-scaling
  run_name: scaling_1700M_dino_b
