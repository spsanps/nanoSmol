# =============================================================================
# FINAL Stage 2: Vision-Language SFT — 1.7B
# =============================================================================
# Model: SmolLM2-1.7B-Instruct + DINOv2-small
# Loss: Answer-only CE (mask user/system tokens)
# LR: Flat 1e-5 all components (1:1, SmolVLM2 style) + cosine decay
# Data: Cauldron (2M images) + all video (~1.6M) + 14% SmolTalk S2 text
# Mix: ~55% image, ~45% video (natural shard ratio), +14% text interleave
# Images: Replicated to 8 frames (A8 sweep winner)
# Init: Best Stage 1 checkpoint
#
# LR scaling: 135M used 3e-5 -> 1.7B: 3e-5 * 0.28 ~ 1e-5
# Memory: A100 80GB — fits without gradient checkpointing
# =============================================================================

stage: 2

model:
  llm: /workspace/models/SmolLM2-1.7B-Instruct
  dino: /workspace/models/dinov2-small
  deep_query: true
  query_dim: 384
  visual_scale: 0.14
  lambda_coarse: 0.0
  gradient_checkpointing: false
  init_from: /workspace/checkpoints/final_1.7B/stage1/best.pt

data:
  train_shards:
    - "/workspace/data/cauldron_full/*.tar"
    - "/workspace/data/openvid/*.tar"
    - "/workspace/data/webvid/*.tar"
    - "/workspace/data/vista_shards/*.tar"
    - "/workspace/data/vista_extra_shards/*.tar"
    - "/workspace/data/vript_long_shards/*.tar"
    - "/workspace/data/vript_shards/*.tar"
    - "/workspace/data/sharegpt4video_shards/*.tar"
    - "/workspace/data/stage3_youtube/*.tar"
  # No val_shards — pretraining-style, train loss only
  text_shards: "/workspace/data/text_retention/stage2/*.tar"
  text_ratio: 0.14
  max_frames: 64
  frame_size: 224
  num_workers: 4
  prefetch_factor: 2
  replicate_image_frames: 8

training:
  total_samples: 1_000_000
  batch_size: 16
  grad_accum: 2
  lr_connector: 1.0e-5
  lr_dino: 1.0e-5
  lr_llm: 1.0e-5
  warmup_ratio: 0.03
  weight_decay: 0.01
  max_grad_norm: 1.0
  schedule: cosine
  dtype: bfloat16
  compile: true
  seed: 42

loss:
  type: text_ce_answer_only

checkpoint:
  save_dir: /workspace/checkpoints/final_1.7B/stage2
  save_every_steps: 1000
  keep_last: 2
  keep_best: 1
  metric: train_loss
  resume: auto

# No eval — pretraining-style, train loss only

wandb:
  project: foveated-vlm-final
  run_name: stage2-1.7B
