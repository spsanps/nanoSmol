# =============================================================================
# FINAL Stage 3: Video SFT (Temporal Reasoning)
# =============================================================================
# Purpose: Temporal reasoning, narrative understanding, video QA.
# Loss: Answer-only CE.
# Init: Best Stage 2 checkpoint.
#
# LR STRATEGY (flat 1:1, SmolVLM2 style):
#   All groups at 3e-5 with cosine decay. Same as Stage 2.
#
# EXCLUDED: LLaVA-Video-178K (67% hit 64-frame cap â†’ noisy temporal signal).
# Data mix: video-heavy (D1 ablation winner for Stage 1, assumed to transfer).
# =============================================================================

stage: 3

model:
  llm: /workspace/models/${MODEL}
  dino: /workspace/models/dinov2-small
  deep_query: true
  query_dim: 384
  visual_scale: 0.14
  lambda_coarse: 0.0
  gradient_checkpointing: false
  init_from: /workspace/checkpoints/final/stage2/best.pt

data:
  train_shards:
    # Video sources
    - "/workspace/data/vista_shards/*.tar"         # VISTA main: 157K
    - "/workspace/data/vista_extra_shards/*.tar"   # VISTA extra: 58K
    - "/workspace/data/sharegpt4video_shards/*.tar" # ShareGPT4Video: 21K
    - "/workspace/data/stage3_youtube/*.tar"        # LLaVA YouTube: 22K
    - "/workspace/data/vript_long_shards/*.tar"    # Vript long captions
    - "/workspace/data/vript_shards/*.tar"          # Vript short: 11K
    # Image source (retain image capability)
    - "/workspace/data/cauldron_full/*.tar"
  val_shards: "/workspace/data/eval/val_10k/*.tar"
  text_shards: "/workspace/data/text_retention/stage3/*.tar"
  text_ratio: 0.14
  max_frames: 64
  frame_size: 224
  num_workers: 2                           # tune per hardware
  prefetch_factor: 2
  replicate_image_frames: 8               # for Cauldron images in mix

training:
  total_samples: 500_000
  batch_size: 8
  grad_accum: 4
  lr_connector: 3.0e-5                     # flat 1:1 (SmolVLM2 style)
  lr_dino: 3.0e-5
  lr_llm: 3.0e-5
  warmup_ratio: 0.03
  weight_decay: 0.01
  max_grad_norm: 1.0
  schedule: cosine
  dtype: bfloat16
  compile: false
  seed: 42

loss:
  type: text_ce_answer_only

checkpoint:
  save_dir: /workspace/checkpoints/final/stage3
  save_every_steps: 1000
  keep_last: 2
  keep_best: 1
  metric: val_loss
  resume: auto

eval:
  every_steps: 250
  max_samples: 1000

wandb:
  project: foveated-vlm-final
  run_name: stage3-${MODEL_SIZE}
